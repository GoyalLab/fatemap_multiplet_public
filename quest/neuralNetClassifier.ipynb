{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training a doublet neural network classifier on DNA barcoded \"ground truth\" singlets for Zhang Melzer et al. 2024\n",
    "### Created by Madeline E Melzer on 20240120\n",
    "### Last edited by Madeline E Melzer on 20240120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'anndata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01manndata\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'anndata'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import anndata\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: failed\n",
      "\n",
      "CondaUpgradeError: This environment has previously been operated on by a conda version that's newer\n",
      "than the conda currently being used. A newer version of conda is required.\n",
      "  target environment location: /software/qanalytics/jupyterhub/env_downtime_2023\n",
      "  current conda version: 4.5.12\n",
      "  minimum conda version: 23.1\n",
      "\n",
      "Update conda and try again.\n",
      "    $ conda install -p \"/software/anaconda3/2018.12\" \"conda>=23.1\"\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: failed\n",
      "\n",
      "InvalidVersionSpecError: Invalid version spec: =2.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -p \"/software/anaconda3/2018.12\" \"conda>=23.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(counts_dir, labels_dir):\n",
    "    os.chdir(counts_dir)\n",
    "\n",
    "    # Read the .mtx file\n",
    "    matrix = scipy.io.mmread(\"matrix.mtx\")\n",
    "    matrix = matrix.transpose()\n",
    "    matrix_array = matrix.toarray()\n",
    "\n",
    "    # Read the features and barcodes files\n",
    "    genes = pd.read_csv(\"features.tsv.gz\", header=None, sep=\"\\t\")\n",
    "    genes_list = genes.index.tolist()\n",
    "    first_column = genes.columns[0]\n",
    "    genes = genes.set_index(first_column) #this is to ensure the anndata object is created correctly and that there are no extra columns in the features or barcodes dfs\n",
    "    barcodes = pd.read_csv(\"barcodes.tsv.gz\", header=None, sep=\"\\t\")\n",
    "    first_column = barcodes.columns[0]\n",
    "    barcodes = barcodes.set_index(first_column)\n",
    "\n",
    "    # Create the AnnData object\n",
    "    data = anndata.AnnData(X=matrix_array, var=genes_list, obs=barcodes)\n",
    "\n",
    "    # getting singlet and multiplet labels\n",
    "    os.chdir(labels_dir)\n",
    "    labels_df = pd.read_csv(f'labels_2.csv') #for \"variable doublet rates\", indicate _{dataset} here\n",
    "\n",
    "    ############## Preprocessing data\n",
    "\n",
    "    # combining features matrix and labels\n",
    "    data.obs.index = data.obs.index.rename('barcode')\n",
    "    merged = data.obs.merge(labels_df, on='barcode', how='inner')\n",
    "    #print(merged.head()) #checking what the merged looks like\n",
    "\n",
    "    # Extract the features matrix and labels\n",
    "    features = data.X\n",
    "    labels = merged['label'].values\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    print(dict(zip(unique_labels, counts))) #checking the number of singlets and multiplets\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "    labels_encoded = 1 - labels_encoded #switching the labels so that 1s are multiplets and 0s are singlets, so correclty identified 1s are considered the positive class\n",
    "    counts = np.bincount(labels_encoded)\n",
    "    #print(counts) #checking that the number of singlets and multiplets is the same as above\n",
    "\n",
    "    barcodes_1 = data.obs.index.to_numpy() #getting the barcodes for the features matrix to identify the cells that are being classified\n",
    "\n",
    "    return features, labels_encoded, barcodes_1, matrix_array, genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doublet': 489, 'singlet': 4401}\n"
     ]
    }
   ],
   "source": [
    "dataset = \"sample1\"\n",
    "\n",
    "counts_dir = os.path.join(\"/Users/mem3579/Library/CloudStorage/OneDrive-NorthwesternUniversity/Arispe and Goyal Labs/ZhangMelzerEtAl/data/classifier/s1s2\", dataset, \"10X_doublets_2/\")\n",
    "labels_dir = os.path.join(\"/Users/mem3579/Library/CloudStorage/OneDrive-NorthwesternUniversity/Arispe and Goyal Labs/ZhangMelzerEtAl/data/classifier/s1s2\", dataset)\n",
    "\n",
    "features, labels_encoded, barcodes_1, matrix_array, genes = load_and_preprocess_data(counts_dir, labels_dir)\n",
    "\n",
    "X = features\n",
    "Y = labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    " # create model\n",
    " model = Sequential()\n",
    " model.add(Dense(3789, input_shape=(3789,), activation='relu'))\n",
    " model.add(Dense(1894, activation='relu'))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    " # Compile model\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', AUC(curve = 'ROC'), AUC(curve='PR')])\n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(model=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    recall = true_positives / (all_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def auc_pr(y_true, y_pred):\n",
    "    auc = tf.metrics.AUC(y_true, y_pred, curve='PR')[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
