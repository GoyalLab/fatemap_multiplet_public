{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the features to see if the structure of the data is meaningful for the classifier.\n",
    "shuffled_indices = np.random.permutation(matrix_array.shape[1]) # Generate a shuffled index\n",
    "matrix_array_shuffled = matrix_array[:, shuffled_indices] # Shuffle the columns of the matrix\n",
    "features_shuffled = features.iloc[shuffled_indices] # Reorder the features DataFrame to match the new column order\n",
    "data_shuffled = anndata.AnnData(X=matrix_array_shuffled, var=features_shuffled, obs=barcodes) # Now, you can create the AnnData object with the shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking randomness in the shuffle (10 repeats)\n",
    "\n",
    "#set working directory to where the labels and \n",
    "os.chdir(counts_dir)\n",
    "\n",
    "# Read the .mtx file\n",
    "matrix = scipy.io.mmread(\"matrix.mtx\")\n",
    "matrix = matrix.transpose()\n",
    "matrix_array = matrix.toarray()\n",
    "\n",
    "# Flatten the matrix, shuffle the values, then reshape it back\n",
    "flattened_matrix = matrix_array.flatten()\n",
    "\n",
    "for repeat in range(10):\n",
    "    np.random.seed(repeat)\n",
    "    np.random.shuffle(flattened_matrix)\n",
    "    scrambled_matrix_array = flattened_matrix.reshape(matrix_array.shape)\n",
    "\n",
    "    os.chdir(counts_dir)\n",
    "    # Read the features and barcodes files\n",
    "    features = pd.read_csv(\"features.tsv.gz\", header=None, sep=\"\\t\")\n",
    "    first_column = features.columns[0]\n",
    "    features = features.set_index(first_column) #this is to ensure the anndata object is created correctly and that there are no extra columns in the features or barcodes dfs\n",
    "    barcodes = pd.read_csv(\"barcodes.tsv.gz\", header=None, sep=\"\\t\")\n",
    "    first_column = barcodes.columns[0]\n",
    "    barcodes = barcodes.set_index(first_column)\n",
    "\n",
    "    # Create the AnnData object\n",
    "    data = anndata.AnnData(X=matrix_array, var=features, obs=barcodes)\n",
    "\n",
    "    # getting singlet and multiplet labels\n",
    "    os.chdir(labels_dir)\n",
    "    labels_df = pd.read_csv('labels_sample1.csv')\n",
    "\n",
    "    ############## Preprocessing data\n",
    "\n",
    "    # combining features matrix and labels\n",
    "    data.obs.index = data.obs.index.rename('barcode')\n",
    "    merged = data.obs.merge(labels_df, on='barcode', how='inner')\n",
    "    print(merged.head()) #checking what the merged looks like\n",
    "\n",
    "    # Extract the features matrix and labels\n",
    "    features = data.X\n",
    "    labels = merged['label'].values\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    print(dict(zip(unique_labels, counts))) #checking the number of singlets and multiplets\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "    labels_encoded = 1 - labels_encoded #switching the labels so that 1s are multiplets and 0s are singlets, so correclty identified 1s are considered true positives\n",
    "    counts = np.bincount(labels_encoded)\n",
    "    print(counts) #checking that the number of singlets and multiplets is the same as above\n",
    "\n",
    "    barcodes_1 = data.obs.index.to_numpy() #getting the barcodes for the features matrix to identify the cells that are being classified\n",
    "\n",
    "    # Define the hyperparameter space\n",
    "    space_tree = {\n",
    "        'n_estimators': hp.choice('n_estimators', range(1, 100)),\n",
    "        'max_depth': hp.choice('max_depth', range(1, 20)),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 1),\n",
    "        'objective': 'binary:logistic',\n",
    "        'min_child_weight': hp.choice('min_child_weight', range(1, 10)),\n",
    "        'gamma': hp.uniform('gamma', 0.1, 1.0),\n",
    "        'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "        'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 1.0, 3.0),\n",
    "        'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 100),\n",
    "        'booster': 'gbtree'\n",
    "    }\n",
    "\n",
    "    # Define objective function\n",
    "    def objective(params):\n",
    "        bst = XGBClassifier(**params, random_state=23)\n",
    "        bst.fit(X_train, y_train)\n",
    "        preds = bst.predict(X_test)\n",
    "        preds_proba = bst.predict_proba(X_test)[:, 1]\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        auroc = roc_auc_score(y_test, preds_proba)  # Calculate AUROC\n",
    "        auprc = average_precision_score(y_test, preds_proba)  # Calculate AUPRC\n",
    "        return {'loss': -auprc, 'accuracy': accuracy, 'status': STATUS_OK, 'auroc': auroc, 'auprc': auprc}\n",
    "\n",
    "    # Run the hyperparameter optimization\n",
    "    trials_tree = Trials()\n",
    "    best_tree = fmin(fn=objective, space=space_tree, algo=tpe.suggest, max_evals=10, trials=trials_tree)\n",
    "    print(f\"Best parameters for tree: {best_tree}\")\n",
    "\n",
    "    # Summary of the success of the hyperparameter optimization\n",
    "    best_tree_score = min(trials_tree.results, key=lambda x: x['loss'])\n",
    "    print(f\"Best tree score: {best_tree_score}\")\n",
    "\n",
    "    # Adjusting the hyperparameters\n",
    "    best_params_tree = {\n",
    "        'n_estimators': best_tree['n_estimators'] + 1,  # +1 because hp.choice returns an index\n",
    "        'max_depth': best_tree['max_depth'] + 1,        # +1 for the same reason\n",
    "        'learning_rate': best_tree['learning_rate'],\n",
    "        'objective': 'binary:logistic',\n",
    "        'min_child_weight': best_tree['min_child_weight'] + 1,  # Adjust if needed\n",
    "        'gamma': best_tree['gamma'],\n",
    "        'subsample': best_tree['subsample'],\n",
    "        'reg_alpha': best_tree['reg_alpha'],\n",
    "        'reg_lambda': best_tree['reg_lambda'],\n",
    "        'scale_pos_weight': best_tree['scale_pos_weight'],\n",
    "        'booster': 'gbtree'\n",
    "    }\n",
    "\n",
    "    # Retrain the classifier with the best hyperparameters\n",
    "    bst_best = XGBClassifier(**best_params_tree, random_state=23)\n",
    "    dump(bst_best, classifiers_dir + f'{dataset}.joblib') #saving unfit classifier\n",
    "    bst_best.fit(X_train, y_train)\n",
    "\n",
    "    preds_proba = bst_best.predict_proba(X_test)[:,1]  # Get probabilities of the positive class (multiplets- 1)\n",
    "    auroc = roc_auc_score(y_test, preds_proba) # Calculate AUROC\n",
    "    print(f\"AUROC: {auroc}\")\n",
    "    auprc = average_precision_score(y_test, preds_proba) # Calculate AUPRC\n",
    "    print(f\"AUPRC: {auprc}\")\n",
    "    y_preds = bst_best.predict(X_test) # Predict labels on the test set\n",
    "    accuracy = accuracy_score(y_test, y_preds) # Calculate accuracy\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
